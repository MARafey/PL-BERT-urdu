{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from phonemize import phonemize\n",
    "import phonemizer\n",
    "import spacy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import yaml\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize phonemizer and tokenizer\n",
    "config_path = \"Configs/config.yml\"\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language='ur', \n",
    "    preserve_punctuation=True,  \n",
    "    with_stress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize spacy tokenizer with GPU support if available\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.blank('ur')\n",
    "if torch.cuda.is_available():\n",
    "    nlp.to(device)\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"Custom Dataset class for batch processing\"\"\"\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "def batch_process_texts(batch_texts):\n",
    "    \"\"\"Process a batch of texts in parallel\"\"\"\n",
    "    processed_batch = []\n",
    "    \n",
    "    # Use GPU for spaCy processing if available\n",
    "    docs = list(nlp.pipe(batch_texts, batch_size=32))\n",
    "    \n",
    "    for doc, text in zip(docs, batch_texts):\n",
    "        # Tokenize the sentence\n",
    "        tokens = [token.text for token in doc]\n",
    "        \n",
    "        # Phonemize the sentence\n",
    "        phonemized = phonemize(text, backend=global_phonemizer)\n",
    "        \n",
    "        processed_batch.append({\n",
    "            'text': text,\n",
    "            'tokens': tokens,\n",
    "            'phonemized': phonemized\n",
    "        })\n",
    "    \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard(shard):\n",
    "    \"\"\"\n",
    "    Process a shard of the dataset using batch processing\n",
    "    \"\"\"\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TextDataset(shard)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    processed_data = []\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        batch_results = batch_process_texts(batch)\n",
    "        processed_data.extend(batch_results)\n",
    "    \n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(path, num_shards=4):\n",
    "    \"\"\"\n",
    "    Process all text files in the given directory with GPU acceleration\n",
    "    \"\"\"\n",
    "    # Getting all files in the directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    Data_Collected = {}\n",
    "    \n",
    "    print(\"Reading input files...\")\n",
    "    for file in tqdm(files):\n",
    "        # If txt just read and save data\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                Data_Collected[file] = data\n",
    "        # If csv read and save data\n",
    "        elif file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(path, file), delimiter='\\t', encoding='utf-8')\n",
    "            Data_Collected[file] = df['summery'] + ' ' + df['title']\n",
    "        # If excel read and save data\n",
    "        elif file.endswith('.xlsx'):\n",
    "            df = pd.read_excel(os.path.join(path, file))\n",
    "            Data_Collected[file] = df['Text']\n",
    "            # Removing all emojis from the text\n",
    "            Data_Collected[file] = Data_Collected[file].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "    \n",
    "    # Combining all to one list\n",
    "    dataset = []\n",
    "    for key in Data_Collected.keys():\n",
    "        if isinstance(Data_Collected[key], pd.DataFrame):\n",
    "            dataset.extend(Data_Collected[key].values.tolist())\n",
    "        elif isinstance(Data_Collected[key], pd.Series):\n",
    "            dataset.extend(Data_Collected[key].tolist())\n",
    "        else:\n",
    "            dataset.append(Data_Collected[key])\n",
    "    \n",
    "    print(f\"Total number of texts to process: {len(dataset)}\")\n",
    "    \n",
    "    # Split the dataset into shards\n",
    "    shard_size = len(dataset) // num_shards\n",
    "    shards = [dataset[i:i+shard_size] for i in range(0, len(dataset), shard_size)]\n",
    "    \n",
    "    # Process each shard in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=num_shards) as executor:\n",
    "        processed_data = list(executor.map(process_shard, shards))\n",
    "    \n",
    "    # Flatten the processed data\n",
    "    processed_data = [item for shard in processed_data for item in shard]\n",
    "    \n",
    "    return processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_dataset(processed_data, output_path):\n",
    "    \"\"\"\n",
    "    Save the processed dataset\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    \n",
    "    print(f'Dataset saved to {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token_mapping(processed_data, config):\n",
    "    \"\"\"\n",
    "    Process token mapping for the dataset with GPU acceleration\n",
    "    \"\"\"\n",
    "    special_token = config['dataset_params']['word_separator']\n",
    "    \n",
    "    # Get all unique tokens\n",
    "    print(\"Collecting unique tokens...\")\n",
    "    unique_tokens = {special_token}\n",
    "    for item in tqdm(processed_data):\n",
    "        if isinstance(item, dict):\n",
    "            for token in item.get('tokens', []):\n",
    "                unique_tokens.add(token)\n",
    "    \n",
    "    unique_tokens = list(unique_tokens)\n",
    "    \n",
    "    # Process tokens in batches for GPU efficiency\n",
    "    print(\"Processing token cases...\")\n",
    "    token_maps = {}\n",
    "    batch_size = 1000\n",
    "    \n",
    "    for i in tqdm(range(0, len(unique_tokens), batch_size)):\n",
    "        batch_tokens = unique_tokens[i:i+batch_size]\n",
    "        # Process batch of tokens\n",
    "        words = [tokenizer.decode([t]) for t in batch_tokens]\n",
    "        words = [word.lower() for word in words]\n",
    "        new_tokens = [tokenizer.encode(word)[0] for word in words]\n",
    "        \n",
    "        for t, word, new_t in zip(batch_tokens, words, new_tokens):\n",
    "            token_maps[t] = {'word': word, 'token': new_t}\n",
    "    \n",
    "    # Save token mapping\n",
    "    with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "        pickle.dump(token_maps, handle)\n",
    "    print(f'Token mapper saved to {config[\"dataset_params\"][\"token_maps\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process dataset\n",
    "input_path = 'Data/To Use'\n",
    "processed_data = process_dataset(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "output_path = os.path.join(config['data_folder'], 'processed_dataset.pkl')\n",
    "save_processed_dataset(processed_data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process token mapping\n",
    "process_token_mapping(processed_data, config)\n",
    "print(\"Processing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
