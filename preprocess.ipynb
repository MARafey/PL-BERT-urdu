{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": "# Notebook for preprocessing Online (Urdu) dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:22:33.660924Z",
     "start_time": "2024-10-27T10:22:33.650532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "from phonemize import phonemize\n",
    "import phonemizer\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ],
   "id": "1b671040f7d3dafb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:22:33.942981Z",
     "start_time": "2024-10-27T10:22:33.676644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize phonemizer and tokenizer\n",
    "config_path = \"Configs/config.yml\"\n",
    "config = yaml.safe_load(open(config_path))\n",
    "\n",
    "# Initialize phonemizer\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language='ur', \n",
    "    preserve_punctuation=True,  \n",
    "    with_stress=True\n",
    ")"
   ],
   "id": "5c4bcb1a702bebe6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:22:34.050727Z",
     "start_time": "2024-10-27T10:22:33.977418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize spacy tokenizer\n",
    "nlp = spacy.blank('ur')\n",
    "\n",
    "# Use the tokenizer attribute of the nlp object\n",
    "tokenizer = nlp.tokenizer"
   ],
   "id": "6d8821a882e11cdb",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T11:02:34.517264Z",
     "start_time": "2024-10-27T11:02:34.483266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataset(path):\n",
    "    \"\"\"\n",
    "    Process all text files in the given directory and perform phonemization\n",
    "    \"\"\"\n",
    "    # Getting all files in the directory\n",
    "    files = os.listdir(path)\n",
    "    \n",
    "    Data_Collected = {}\n",
    "    \n",
    "    print(\"Reading input files...\")\n",
    "    for file in tqdm(files):\n",
    "        # If txt just read and save data\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as f:\n",
    "                data = f.read()\n",
    "                Data_Collected[file] = data\n",
    "        # If csv read and save data\n",
    "        elif file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(path, file), delimiter='\\t', encoding='utf-8')\n",
    "            Data_Collected[file] = df\n",
    "        # If excel read and save data\n",
    "        elif file.endswith('.xlsx'):\n",
    "            df = pd.read_excel(os.path.join(path, file))\n",
    "            Data_Collected[file] = df['Text']\n",
    "            # Removing all emojis from the text\n",
    "            Data_Collected[file] = Data_Collected[file].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "    \n",
    "    # Combining all to one list\n",
    "    dataset = []\n",
    "    for key in Data_Collected.keys():\n",
    "        if isinstance(Data_Collected[key], pd.DataFrame):\n",
    "            dataset.extend(Data_Collected[key].values.tolist())\n",
    "        elif isinstance(Data_Collected[key], pd.Series):\n",
    "            dataset.extend(Data_Collected[key].tolist())\n",
    "        else:\n",
    "            dataset.append(Data_Collected[key])\n",
    "    \n",
    "    print(f\"Total number of texts to process: {len(dataset)}\")\n",
    "    \n",
    "    import re\n",
    "    # Process the entire dataset\n",
    "    processed_data = []\n",
    "    print(\"Processing texts with phonemizer...\")\n",
    "    for text in tqdm(dataset):\n",
    "        if isinstance(text, str):  # Only process if it's a string\n",
    "            nlp = spacy.blank('ur')\n",
    "                \n",
    "            # chunking the text into smaller parts\n",
    "            if len(text) > 1000:\n",
    "                processed = []\n",
    "                # splitting on the occurence of full stops or new lines or '؟'\n",
    "                chunks = re.split(r'[.؟\\n]', text)\n",
    "                for chunk in chunks:\n",
    "                    if len(chunk) > 0:\n",
    "                        processed.append(nlp(chunk))\n",
    "                processed_data.append(processed)\n",
    "            else:\n",
    "                processed = nlp(text)\n",
    "            processed_data.append(processed)\n",
    "    \n",
    "    return processed_data"
   ],
   "id": "a4e4d7b111dbecb1",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-27T11:02:35.002489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process dataset\n",
    "input_path = 'Data/To Use'\n",
    "processed_data = process_dataset(input_path)"
   ],
   "id": "cc7e6dd99e04006",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:22:34.145327Z",
     "start_time": "2024-10-27T10:22:34.133799Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 30,
   "source": [
    "def save_processed_dataset(processed_data, output_path):\n",
    "    \"\"\"\n",
    "    Save the processed dataset\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "    \n",
    "    print(f'Dataset saved to {output_path}')"
   ],
   "id": "637025f911e60b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T10:22:34.175934Z",
     "start_time": "2024-10-27T10:22:34.163899Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 31,
   "source": [
    "def process_token_mapping(processed_data, config):\n",
    "    \"\"\"\n",
    "    Process token mapping for the dataset\n",
    "    \"\"\"\n",
    "    special_token = config['dataset_params']['word_separator']\n",
    "    \n",
    "    # Get all unique tokens\n",
    "    print(\"Collecting unique tokens...\")\n",
    "    unique_tokens = {special_token}\n",
    "    for item in tqdm(processed_data):\n",
    "        if isinstance(item, dict):  # Ensure the item is a dictionary\n",
    "            for token in item.get('tokens', []):\n",
    "                unique_tokens.add(token)\n",
    "    \n",
    "    unique_tokens = list(unique_tokens)\n",
    "    \n",
    "    # Get lower case tokens\n",
    "    print(\"Processing token cases...\")\n",
    "    token_maps = {}\n",
    "    for t in tqdm(unique_tokens):\n",
    "        word = tokenizer.decode([t])\n",
    "        word = word.lower()\n",
    "        new_t = tokenizer.encode(word)[0]\n",
    "        token_maps[t] = {'word': word, 'token': new_t}\n",
    "    \n",
    "    # Save token mapping\n",
    "    with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "        pickle.dump(token_maps, handle)\n",
    "    print(f'Token mapper saved to {config[\"dataset_params\"][\"token_maps\"]}')"
   ],
   "id": "8fa88c3a59661f1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save processed dataset\n",
    "output_path = os.path.join(config['data_folder'], 'processed_dataset.pkl')\n",
    "save_processed_dataset(processed_data, output_path)"
   ],
   "id": "b374bd1490aa4031",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process token mapping\n",
    "process_token_mapping(processed_data, config)"
   ],
   "id": "1407167cfc5eccc9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Processing complete!\")",
   "id": "3257bc5bb6dbb9f3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
